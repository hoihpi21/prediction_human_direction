{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing of UWB BLE Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "import re\n",
    "from pykalman import KalmanFilter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Thai to English translation mapping\n",
    "thai_to_english_mappings = {\n",
    "    # Location zones (inExpectedZoneName)\n",
    "    'โถงกลางชั้น 1': 'Central Hall Floor 1',\n",
    "    'ล็อบบี้ ชั้น 1': 'Lobby Floor 1',\n",
    "    'ห้องกระจก ชั้น 1': 'Glass Room Floor 1',\n",
    "    'ทางเดินชั้น1': 'Corridor Floor 1',\n",
    "    'ห้องน้ำชายด้านหลัง': 'Male Restroom Back Area',\n",
    "    'ลิฟท์ด้านหน้า': 'Front Elevator',\n",
    "    'ทางเดินชั้น3ด้านหน้า': 'Front Corridor Floor 3',\n",
    "    'ทางเดินชั้น2': 'Corridor Floor 2',\n",
    "    \n",
    "    # Group names (group_name)\n",
    "    'ทีมวิจัยอิเล็กทรอนิกส์สำหรับนวัตกรรมไร้สาย': 'Electronics Research Team for Wireless Innovation',\n",
    "    'กลุ่มงานประเมินองค์กร': 'Organization Assessment Team'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 1: Data Loading and Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_paths):\n",
    "    \"\"\"\n",
    "    Load data from multiple CSV files, remove unnecessary columns,\n",
    "    translate Thai text to English and combine datasets.\n",
    "    \n",
    "    Args:\n",
    "        file_paths (list): List of paths to CSV files\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined preprocessed dataframe\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        # Load data\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add source file column for traceability\n",
    "        df['source_file'] = os.path.basename(file_path)\n",
    "        \n",
    "        # Function to check if text contains Thai characters\n",
    "        def contains_thai(text):\n",
    "            if not isinstance(text, str):\n",
    "                return False\n",
    "            thai_pattern = r'[\\u0E00-\\u0E7F]'\n",
    "            import re\n",
    "            return bool(re.search(thai_pattern, text))\n",
    "        \n",
    "        # Identify all columns that contain Thai text\n",
    "        thai_cols = []\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':  # Check only string columns\n",
    "                if df[col].apply(lambda x: contains_thai(x) if isinstance(x, str) else False).any():\n",
    "                    thai_cols.append(col)\n",
    "        \n",
    "        if thai_cols:\n",
    "            print(f\"Found Thai text in these columns in {file_path}: {thai_cols}\")\n",
    "        \n",
    "        # Translate all columns with Thai text\n",
    "        for col in thai_cols:\n",
    "            df[col] = df[col].apply(lambda x: translate_thai_to_english(x) if isinstance(x, str) else x)\n",
    "        \n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Sort by timestamp to ensure chronological order\n",
    "    combined_df = combined_df.sort_values('timestamp')\n",
    "    \n",
    "    # Create a time-based feature (seconds from first timestamp)\n",
    "    min_timestamp = combined_df['timestamp'].min()\n",
    "    combined_df['time_seconds'] = (combined_df['timestamp'] - min_timestamp) / 1000\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    columns_to_keep = [\n",
    "        'id', 'x', 'y', 'timestamp', 'time_seconds',\n",
    "        'inExpectedZoneName', \n",
    "        'group_name', 'floor', 'inZones'\n",
    "    ]\n",
    "    \n",
    "    # Make sure we only keep columns that actually exist in the dataframe\n",
    "    columns_to_keep = [col for col in columns_to_keep if col in combined_df.columns]\n",
    "    \n",
    "    combined_df = combined_df[columns_to_keep]\n",
    "    \n",
    "    # Check if there are any remaining Thai characters in the processed data\n",
    "    remaining_thai = False\n",
    "    for col in combined_df.columns:\n",
    "        if combined_df[col].dtype == 'object':\n",
    "            if combined_df[col].apply(lambda x: contains_thai(x) if isinstance(x, str) else False).any():\n",
    "                remaining_thai = True\n",
    "                print(f\"WARNING: Column '{col}' still contains Thai characters after translation.\")\n",
    "                # Show examples of untranslated Thai text\n",
    "                thai_examples = combined_df[combined_df[col].apply(\n",
    "                    lambda x: contains_thai(x) if isinstance(x, str) else False\n",
    "                )][col].unique()\n",
    "                print(\"Examples of untranslated Thai text:\")\n",
    "                for example in thai_examples[:5]:  # Show at most 5 examples\n",
    "                    print(f\"  - {example}\")\n",
    "    \n",
    "    if remaining_thai:\n",
    "        print(\"\\nTo fix this issue, add these Thai texts to the thai_to_english_mappings dictionary.\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def translate_thai_to_english(text):\n",
    "    \"\"\"\n",
    "    Translate Thai text to English using predefined mappings\n",
    "    \n",
    "    Args:\n",
    "        text (str): Thai text to translate\n",
    "        \n",
    "    Returns:\n",
    "        str: Translated English text if available, otherwise original text\n",
    "    \"\"\"\n",
    "    return thai_to_english_mappings.get(text, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 2: Outlier Detection and Removal using UQR Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df, columns=['x', 'y', 'z'], multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Remove outliers using the Interquartile Range (IQR) method\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        columns (list): Columns to check for outliers\n",
    "        multiplier (float): IQR multiplier for defining outliers\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with outliers removed\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    outlier_indices = set()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            lower_bound = Q1 - multiplier * IQR\n",
    "            upper_bound = Q3 + multiplier * IQR\n",
    "            \n",
    "            # Identify outliers\n",
    "            col_outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)].index\n",
    "            outlier_indices.update(col_outliers)\n",
    "    \n",
    "    # Print outlier statistics\n",
    "    print(f\"Identified {len(outlier_indices)} outliers out of {len(df)} rows ({len(outlier_indices)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Remove outliers\n",
    "    df_clean = df_clean.drop(index=outlier_indices)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 3: Kalman Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kalman_filtering(df, columns=['x', 'y']):\n",
    "    \"\"\"\n",
    "    Apply Kalman filtering to smooth position data\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        columns (list): Columns to apply Kalman filtering\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with Kalman filtered columns added\n",
    "    \"\"\"\n",
    "    df_kalman = df.copy()\n",
    "    \n",
    "    # For each position dimension, apply Kalman filter\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            # Initial state\n",
    "            initial_state_mean = [df[col].iloc[0], 0]  # Position and velocity\n",
    "            initial_state_covariance = np.eye(2)\n",
    "            \n",
    "            # Transition matrix (position and velocity)\n",
    "            transition_matrix = np.array([[1, 1], [0, 1]])\n",
    "            \n",
    "            # Observation matrix (we observe only position)\n",
    "            observation_matrix = np.array([[1, 0]])\n",
    "            \n",
    "            # Process noise (uncertainty in our model)\n",
    "            process_noise = np.eye(2) * 0.01\n",
    "            \n",
    "            # Observation noise (uncertainty in our measurements)\n",
    "            observation_noise = 1.0\n",
    "            \n",
    "            # Create Kalman filter\n",
    "            kf = KalmanFilter(\n",
    "                initial_state_mean=initial_state_mean,\n",
    "                initial_state_covariance=initial_state_covariance,\n",
    "                transition_matrices=transition_matrix,\n",
    "                observation_matrices=observation_matrix,\n",
    "                transition_covariance=process_noise,\n",
    "                observation_covariance=observation_noise\n",
    "            )\n",
    "            \n",
    "            # Apply filter\n",
    "            smoothed_state_means, _ = kf.smooth(df[col].values)\n",
    "            \n",
    "            # Add filtered values to dataframe\n",
    "            df_kalman[f'{col}_kalman'] = smoothed_state_means[:, 0]\n",
    "    \n",
    "    return df_kalman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 4: Noise Filtering (Media + Low Pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_noise_filtering(df, columns=['x', 'y'], window_size=5, cutoff=0.1):\n",
    "    \"\"\"\n",
    "    Apply median and low-pass filters to remove noise\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        columns (list): Columns to filter\n",
    "        window_size (int): Window size for median filter\n",
    "        cutoff (float): Cutoff frequency for low-pass filter\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with filtered columns added\n",
    "    \"\"\"\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            # Apply median filter\n",
    "            median_filtered = signal.medfilt(df[col].values, window_size)\n",
    "            df_filtered[f'{col}_median'] = median_filtered\n",
    "            \n",
    "            # Apply low-pass filter\n",
    "            # Design Butterworth filter\n",
    "            b, a = signal.butter(3, cutoff, 'low')\n",
    "            # Apply filter\n",
    "            low_pass_filtered = signal.filtfilt(b, a, median_filtered)\n",
    "            df_filtered[f'{col}_lowpass'] = low_pass_filtered\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 5: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df, columns=['x', 'y', 'x_kalman', 'y_kalman', 'x_lowpass', 'y_lowpass']):\n",
    "    \"\"\"\n",
    "    Normalize data using Min-Max scaling\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        columns (list): Columns to normalize\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with normalized columns added\n",
    "    \"\"\"\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            range_val = max_val - min_val\n",
    "            \n",
    "            if range_val > 0:\n",
    "                df_norm[f'{col}_norm'] = (df[col] - min_val) / range_val\n",
    "            else:\n",
    "                df_norm[f'{col}_norm'] = df[col]  # No normalization if range is 0\n",
    "    \n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 6: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer additional features from the data\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with engineered features added\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Calculate velocity between consecutive positions (using Kalman-filtered positions)\n",
    "    if 'x_kalman' in df.columns and 'y_kalman' in df.columns and 'time_seconds' in df.columns:\n",
    "        # Calculate position change\n",
    "        df_features['dx'] = df_features['x_kalman'].diff()\n",
    "        df_features['dy'] = df_features['y_kalman'].diff()\n",
    "        \n",
    "        # Calculate time difference\n",
    "        df_features['dt'] = df_features['time_seconds'].diff()\n",
    "        \n",
    "        # Calculate velocity (where dt > 0 to avoid division by zero)\n",
    "        mask = df_features['dt'] > 0\n",
    "        df_features.loc[mask, 'velocity_x'] = df_features.loc[mask, 'dx'] / df_features.loc[mask, 'dt']\n",
    "        df_features.loc[mask, 'velocity_y'] = df_features.loc[mask, 'dy'] / df_features.loc[mask, 'dt']\n",
    "        \n",
    "        # Calculate speed (magnitude of velocity)\n",
    "        df_features['speed'] = np.sqrt(df_features['velocity_x']**2 + df_features['velocity_y']**2)\n",
    "        \n",
    "        # Calculate acceleration\n",
    "        df_features['acceleration_x'] = df_features['velocity_x'].diff() / df_features['dt']\n",
    "        df_features['acceleration_y'] = df_features['velocity_y'].diff() / df_features['dt']\n",
    "        df_features['acceleration'] = np.sqrt(df_features['acceleration_x']**2 + df_features['acceleration_y']**2)\n",
    "    \n",
    "    # Calculate distance from origin\n",
    "    if 'x_kalman' in df.columns and 'y_kalman' in df.columns:\n",
    "        df_features['distance_from_origin'] = np.sqrt(df_features['x_kalman']**2 + df_features['y_kalman']**2)\n",
    "    \n",
    "    # Create time-based features\n",
    "    if 'timestamp' in df.columns:\n",
    "        # Convert timestamp to datetime\n",
    "        df_features['datetime'] = pd.to_datetime(df_features['timestamp'], unit='ms')\n",
    "        df_features['hour'] = df_features['datetime'].dt.hour\n",
    "        df_features['minute'] = df_features['datetime'].dt.minute\n",
    "        df_features['second'] = df_features['datetime'].dt.second\n",
    "        df_features['day_of_week'] = df_features['datetime'].dt.dayofweek\n",
    "    \n",
    "    # Calculate distance between consecutive points\n",
    "    if 'x_kalman' in df.columns and 'y_kalman' in df.columns:\n",
    "        df_features['distance'] = np.sqrt(df_features['dx']**2 + df_features['dy']**2)\n",
    "        \n",
    "        # Calculate cumulative distance traveled\n",
    "        df_features['cumulative_distance'] = df_features['distance'].cumsum()\n",
    "    \n",
    "    # Calculate direction/angle of movement (in radians)\n",
    "    if 'dx' in df_features.columns and 'dy' in df_features.columns:\n",
    "        df_features['direction'] = np.arctan2(df_features['dy'], df_features['dx'])\n",
    "        \n",
    "        # Convert to degrees for easier interpretation\n",
    "        df_features['direction_degrees'] = df_features['direction'] * 180 / np.pi\n",
    "    \n",
    "    return df_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 7: Visualization and Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories(df, output_path='trajectory_plot.png'):\n",
    "    \"\"\"\n",
    "    Plot the original, Kalman filtered, and low-pass filtered trajectories\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        output_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot original data\n",
    "    plt.scatter(df['x'], df['y'], color='blue', alpha=0.3, label='Original')\n",
    "    \n",
    "    # Plot Kalman filtered data if available\n",
    "    if 'x_kalman' in df.columns and 'y_kalman' in df.columns:\n",
    "        plt.plot(df['x_kalman'], df['y_kalman'], color='red', linewidth=2, label='Kalman Filtered')\n",
    "    \n",
    "    # Plot low-pass filtered data if available\n",
    "    if 'x_lowpass' in df.columns and 'y_lowpass' in df.columns:\n",
    "        plt.plot(df['x_lowpass'], df['y_lowpass'], color='green', linewidth=2, label='Low-pass Filtered')\n",
    "    \n",
    "    plt.title('Position Trajectories: Original vs Filtered')\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved trajectory plot to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 1: Loading and preprocessing data...\n",
      "Found Thai text in these columns in 3.4.2025.csv: ['group_name']\n",
      "Found Thai text in these columns in 6.5.2025(1).csv: ['group_name']\n",
      "Found Thai text in these columns in 6.5.2025(2).csv: ['group_name']\n",
      "Combined data shape: (1453, 9)\n",
      "\n",
      "STAGE 2: Removing outliers...\n",
      "Identified 169 outliers out of 1453 rows (11.63%)\n",
      "Data shape after outlier removal: (1284, 9)\n",
      "\n",
      "STAGE 3: Applying Kalman filtering...\n",
      "\n",
      "STAGE 4: Applying noise filtering...\n",
      "\n",
      "STAGE 5: Normalizing data...\n",
      "\n",
      "STAGE 6: Engineering features...\n",
      "Final data shape: (1284, 40)\n",
      "\n",
      "STAGE 7: Visualizing data...\n",
      "Saved trajectory plot to trajectory_plot.png\n",
      "\n",
      "Processed data saved to uwb_preprocessing.csv\n",
      "\n",
      "Summary of processed data:\n",
      "                id            x            y     timestamp  time_seconds  \\\n",
      "count  1284.000000  1284.000000  1284.000000  1.284000e+03  1.284000e+03   \n",
      "mean   5406.158879    27.719597    21.640298  1.745666e+12  2.001728e+06   \n",
      "min     478.000000     7.701000    16.549000  1.743664e+12  1.171150e+02   \n",
      "25%    5410.000000    18.225500    20.450750  1.743667e+12  2.984526e+03   \n",
      "50%    5410.000000    29.577000    21.698500  1.746503e+12  2.839019e+06   \n",
      "75%    5410.000000    33.629500    22.677250  1.746514e+12  2.850594e+06   \n",
      "max    5410.000000    60.239000    26.607000  1.746517e+12  2.853122e+06   \n",
      "std     137.638698    10.112903     1.812986  1.299552e+09  1.299552e+06   \n",
      "\n",
      "        floor     x_kalman     y_kalman     x_median    x_lowpass  ...  \\\n",
      "count  1284.0  1284.000000  1284.000000  1284.000000  1284.000000  ...   \n",
      "mean      1.0    27.720016    21.639068    27.644770    27.652850  ...   \n",
      "min       1.0    10.693297    18.526309     8.710000    10.474682  ...   \n",
      "25%       1.0    18.283452    20.831804    18.283250    18.283062  ...   \n",
      "50%       1.0    29.726673    21.633253    29.563500    29.562464  ...   \n",
      "75%       1.0    33.579611    22.433125    33.396000    33.441189  ...   \n",
      "max       1.0    54.485219    25.971936    58.112000    54.804824  ...   \n",
      "std       0.0     9.547658     1.317372     9.890057     9.533324  ...   \n",
      "\n",
      "       distance_from_origin                       datetime         hour  \\\n",
      "count           1284.000000                           1284  1284.000000   \n",
      "mean              35.657591  2025-04-26 11:05:02.994539008     5.570093   \n",
      "min               21.681915     2025-04-03 07:04:51.934000     3.000000   \n",
      "25%               27.754783  2025-04-03 07:52:39.344499968     3.000000   \n",
      "50%               36.813414  2025-05-06 03:39:53.574499840     6.500000   \n",
      "75%               40.184788  2025-05-06 06:52:48.344499968     7.000000   \n",
      "max               58.801141     2025-05-06 07:34:56.530000     7.000000   \n",
      "std                7.619109                            NaN     1.757445   \n",
      "\n",
      "            minute       second  day_of_week     distance  \\\n",
      "count  1284.000000  1284.000000  1284.000000  1283.000000   \n",
      "mean     30.902648    29.645639     1.593458     0.481279   \n",
      "min       0.000000     0.000000     1.000000     0.002906   \n",
      "25%      17.000000    14.000000     1.000000     0.134723   \n",
      "50%      32.000000    30.000000     1.000000     0.257810   \n",
      "75%      45.000000    45.000000     3.000000     0.512371   \n",
      "max      59.000000    59.000000     3.000000     4.295415   \n",
      "std      16.303121    17.485631     0.913988     0.614856   \n",
      "\n",
      "       cumulative_distance    direction  direction_degrees  \n",
      "count          1283.000000  1283.000000        1283.000000  \n",
      "mean            364.712178    -0.053241          -3.050509  \n",
      "min               3.642066    -3.139252        -179.865874  \n",
      "25%             262.425416    -1.484376         -85.048459  \n",
      "50%             375.633898    -0.015687          -0.898796  \n",
      "75%             499.757407     1.265349          72.499135  \n",
      "max             617.480848     3.138171         179.803958  \n",
      "std             150.986447     1.873386         107.337103  \n",
      "\n",
      "[8 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Define file paths\n",
    "    file_paths = ['3.4.2025.csv', '6.5.2025(1).csv', '6.5.2025(2).csv']\n",
    "    \n",
    "    # Set output path for processed data\n",
    "    output_path = 'uwb_preprocessing.csv'\n",
    "    \n",
    "    # STAGE 1: Load and preprocess data\n",
    "    print(\"STAGE 1: Loading and preprocessing data...\")\n",
    "    df = load_and_preprocess_data(file_paths)\n",
    "    print(f\"Combined data shape: {df.shape}\")\n",
    "    \n",
    "    # STAGE 2: Remove outliers\n",
    "    print(\"\\nSTAGE 2: Removing outliers...\")\n",
    "    df_clean = remove_outliers_iqr(df)\n",
    "    print(f\"Data shape after outlier removal: {df_clean.shape}\")\n",
    "    \n",
    "    # STAGE 3: Apply Kalman filtering\n",
    "    print(\"\\nSTAGE 3: Applying Kalman filtering...\")\n",
    "    df_kalman = apply_kalman_filtering(df_clean)\n",
    "    \n",
    "    # STAGE 4: Apply noise filtering\n",
    "    print(\"\\nSTAGE 4: Applying noise filtering...\")\n",
    "    df_filtered = apply_noise_filtering(df_kalman)\n",
    "    \n",
    "    # STAGE 5: Normalize data\n",
    "    print(\"\\nSTAGE 5: Normalizing data...\")\n",
    "    df_norm = normalize_data(df_filtered)\n",
    "    \n",
    "    # STAGE 6: Engineer features\n",
    "    print(\"\\nSTAGE 6: Engineering features...\")\n",
    "    df_features = engineer_features(df_norm)\n",
    "    print(f\"Final data shape: {df_features.shape}\")\n",
    "    \n",
    "    # STAGE 7: Visualize data\n",
    "    print(\"\\nSTAGE 7: Visualizing data...\")\n",
    "    plot_trajectories(df_features)\n",
    "    \n",
    "    # Save processed data\n",
    "    df_features.to_csv(output_path, index=False)\n",
    "    print(f\"\\nProcessed data saved to {output_path}\")\n",
    "    \n",
    "    # Print summary of processed data\n",
    "    print(\"\\nSummary of processed data:\")\n",
    "    print(df_features.describe())\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for Thai characters in 3.4.2025.csv...\n",
      "  Thai characters found in column 'group_name': ทีมวิจัยอิเล็กทรอนิกส์สำหรับนวัตกรรมไร้สาย\n",
      "  Thai characters found in column 'group_name': กลุ่มงานประเมินองค์กร\n",
      "\n",
      "Checking for Thai characters in 6.5.2025(1).csv...\n",
      "  Thai characters found in column 'group_name': ทีมวิจัยอิเล็กทรอนิกส์สำหรับนวัตกรรมไร้สาย\n",
      "\n",
      "Checking for Thai characters in 6.5.2025(2).csv...\n",
      "  Thai characters found in column 'group_name': ทีมวิจัยอิเล็กทรอนิกส์สำหรับนวัตกรรมไร้สาย\n",
      "==================================================\n",
      "UWB BLE DATA PREPROCESSING PIPELINE\n",
      "==================================================\n",
      "\n",
      "STAGE 1: Loading and preprocessing data...\n",
      "Found Thai text in these columns in 3.4.2025.csv: ['group_name']\n",
      "Found Thai text in these columns in 6.5.2025(1).csv: ['group_name']\n",
      "Found Thai text in these columns in 6.5.2025(2).csv: ['group_name']\n",
      "Combined data shape: (1453, 9)\n",
      "Stage 1 output saved to processed_data/stage1_preprocessed.csv\n",
      "\n",
      "Validating translations in processed_data/stage1_preprocessed.csv...\n",
      "  No Thai characters found in output file. Translation successful!\n",
      "\n",
      "STAGE 2: Removing outliers...\n",
      "Identified 169 outliers out of 1453 rows (11.63%)\n",
      "Data shape after outlier removal: (1284, 9)\n",
      "Stage 2 output saved to processed_data/stage2_outliers_removed.csv\n",
      "\n",
      "STAGE 3: Applying Kalman filtering...\n",
      "Stage 3 output saved to processed_data/stage3_kalman_filtered.csv\n",
      "\n",
      "STAGE 4: Applying noise filtering...\n",
      "Stage 4 output saved to processed_data/stage4_noise_filtered.csv\n",
      "\n",
      "STAGE 5: Normalizing data...\n",
      "Stage 5 output saved to processed_data/stage5_normalized.csv\n",
      "\n",
      "STAGE 6: Engineering features...\n",
      "Final data shape: (1284, 40)\n",
      "\n",
      "STAGE 7: Visualizing data...\n",
      "Saved trajectory plot to processed_data/trajectory_plot.png\n",
      "\n",
      "Final processed data saved to processed_data/uwb_processed_final.csv\n",
      "\n",
      "Validating translations in processed_data/uwb_processed_final.csv...\n",
      "  No Thai characters found in output file. Translation successful!\n",
      "All Thai text successfully translated to English in the final output.\n",
      "\n",
      "PROCESSING SUMMARY:\n",
      "Total processing time: 0.39 seconds\n",
      "Input files: 3\n",
      "Total rows processed: 1453\n",
      "Final output rows: 1284\n",
      "Final output columns: 40\n",
      "\n",
      "Final dataset columns:\n",
      "- acceleration\n",
      "- acceleration_x\n",
      "- acceleration_y\n",
      "- cumulative_distance\n",
      "- datetime\n",
      "- day_of_week\n",
      "- direction\n",
      "- direction_degrees\n",
      "- distance\n",
      "- distance_from_origin\n",
      "- dt\n",
      "- dx\n",
      "- dy\n",
      "- floor\n",
      "- group_name\n",
      "- hour\n",
      "- id\n",
      "- inExpectedZoneName\n",
      "- inZones\n",
      "- minute\n",
      "- second\n",
      "- speed\n",
      "- time_seconds\n",
      "- timestamp\n",
      "- velocity_x\n",
      "- velocity_y\n",
      "- x\n",
      "- x_kalman\n",
      "- x_kalman_norm\n",
      "- x_lowpass\n",
      "- x_lowpass_norm\n",
      "- x_median\n",
      "- x_norm\n",
      "- y\n",
      "- y_kalman\n",
      "- y_kalman_norm\n",
      "- y_lowpass\n",
      "- y_lowpass_norm\n",
      "- y_median\n",
      "- y_norm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "# from uwb_preprocessing import (\n",
    "#     load_and_preprocess_data,\n",
    "#     remove_outliers_iqr,\n",
    "#     apply_kalman_filtering,\n",
    "#     apply_noise_filtering,\n",
    "#     normalize_data,\n",
    "#     engineer_features,\n",
    "#     plot_trajectories,\n",
    "#     translate_thai_to_english\n",
    "# )\n",
    "\n",
    "def check_thai_characters_in_data(file_paths):\n",
    "    \"\"\"\n",
    "    Check if any Thai characters remain in the data\n",
    "    \n",
    "    Args:\n",
    "        file_paths (list): List of paths to CSV files\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"\\nChecking for Thai characters in {file_path}...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        thai_found = False\n",
    "        \n",
    "        # Check each column that might contain Thai characters\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':  # Only check string columns\n",
    "                # Check if any value in this column contains Thai characters\n",
    "                for val in df[col].dropna().unique():\n",
    "                    if isinstance(val, str) and bool(re.search(r'[\\u0E00-\\u0E7F]', val)):\n",
    "                        print(f\"  Thai characters found in column '{col}': {val}\")\n",
    "                        thai_found = True\n",
    "        \n",
    "        if not thai_found:\n",
    "            print(f\"  No Thai characters found in {file_path}\")\n",
    "\n",
    "def validate_translations(output_file):\n",
    "    \"\"\"\n",
    "    Validate that all Thai characters have been translated in the output file\n",
    "    \n",
    "    Args:\n",
    "        output_file (str): Path to the output CSV file\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if all Thai characters have been translated, False otherwise\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(f\"\\nValidating translations in {output_file}...\")\n",
    "    \n",
    "    if not os.path.exists(output_file):\n",
    "        print(f\"  Error: Output file {output_file} does not exist\")\n",
    "        return False\n",
    "    \n",
    "    df = pd.read_csv(output_file)\n",
    "    \n",
    "    thai_found = False\n",
    "    columns_with_thai = []\n",
    "    \n",
    "    # Check each column for Thai characters\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':  # Only check string columns\n",
    "            for val in df[col].dropna().head(1000).values:  # Check first 1000 non-null values\n",
    "                if isinstance(val, str) and bool(re.search(r'[\\u0E00-\\u0E7F]', val)):\n",
    "                    thai_found = True\n",
    "                    if col not in columns_with_thai:\n",
    "                        columns_with_thai.append(col)\n",
    "    \n",
    "    if thai_found:\n",
    "        print(f\"  Thai characters found in output file in columns: {columns_with_thai}\")\n",
    "        print(\"  Translation was incomplete. Please update the translation dictionary.\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"  No Thai characters found in output file. Translation successful!\")\n",
    "        return True\n",
    "\n",
    "def main():\n",
    "    # Define file paths (update these to match your file locations)\n",
    "    file_paths = ['3.4.2025.csv', '6.5.2025(1).csv', '6.5.2025(2).csv']\n",
    "    \n",
    "    # Set output paths\n",
    "    output_dir = 'processed_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    final_output = os.path.join(output_dir, 'uwb_processed_final.csv')\n",
    "    plot_output = os.path.join(output_dir, 'trajectory_plot.png')\n",
    "    \n",
    "    # Check for Thai characters in original data\n",
    "    check_thai_characters_in_data(file_paths)\n",
    "    \n",
    "    # Track processing time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"UWB BLE DATA PREPROCESSING PIPELINE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # STAGE 1: Load and preprocess data\n",
    "    print(\"\\nSTAGE 1: Loading and preprocessing data...\")\n",
    "    df = load_and_preprocess_data(file_paths)\n",
    "    print(f\"Combined data shape: {df.shape}\")\n",
    "    stage1_output = os.path.join(output_dir, 'stage1_preprocessed.csv')\n",
    "    df.to_csv(stage1_output, index=False)\n",
    "    print(f\"Stage 1 output saved to {stage1_output}\")\n",
    "    \n",
    "    # Validate translations\n",
    "    translate_valid = validate_translations(stage1_output)\n",
    "    if not translate_valid:\n",
    "        print(\"Warning: Some Thai characters remain in the data. Consider updating the translation dictionary.\")\n",
    "    \n",
    "    # STAGE 2: Remove outliers\n",
    "    print(\"\\nSTAGE 2: Removing outliers...\")\n",
    "    df_clean = remove_outliers_iqr(df)\n",
    "    print(f\"Data shape after outlier removal: {df_clean.shape}\")\n",
    "    stage2_output = os.path.join(output_dir, 'stage2_outliers_removed.csv')\n",
    "    df_clean.to_csv(stage2_output, index=False)\n",
    "    print(f\"Stage 2 output saved to {stage2_output}\")\n",
    "    \n",
    "    # STAGE 3: Apply Kalman filtering\n",
    "    print(\"\\nSTAGE 3: Applying Kalman filtering...\")\n",
    "    df_kalman = apply_kalman_filtering(df_clean)\n",
    "    stage3_output = os.path.join(output_dir, 'stage3_kalman_filtered.csv')\n",
    "    df_kalman.to_csv(stage3_output, index=False)\n",
    "    print(f\"Stage 3 output saved to {stage3_output}\")\n",
    "    \n",
    "    # STAGE 4: Apply noise filtering\n",
    "    print(\"\\nSTAGE 4: Applying noise filtering...\")\n",
    "    df_filtered = apply_noise_filtering(df_kalman)\n",
    "    stage4_output = os.path.join(output_dir, 'stage4_noise_filtered.csv')\n",
    "    df_filtered.to_csv(stage4_output, index=False)\n",
    "    print(f\"Stage 4 output saved to {stage4_output}\")\n",
    "    \n",
    "    # STAGE 5: Normalize data\n",
    "    print(\"\\nSTAGE 5: Normalizing data...\")\n",
    "    df_norm = normalize_data(df_filtered)\n",
    "    stage5_output = os.path.join(output_dir, 'stage5_normalized.csv')\n",
    "    df_norm.to_csv(stage5_output, index=False)\n",
    "    print(f\"Stage 5 output saved to {stage5_output}\")\n",
    "    \n",
    "    # STAGE 6: Engineer features\n",
    "    print(\"\\nSTAGE 6: Engineering features...\")\n",
    "    df_features = engineer_features(df_norm)\n",
    "    print(f\"Final data shape: {df_features.shape}\")\n",
    "    \n",
    "    # STAGE 7: Visualize data\n",
    "    print(\"\\nSTAGE 7: Visualizing data...\")\n",
    "    plot_trajectories(df_features, output_path=plot_output)\n",
    "    \n",
    "    # Save final processed data\n",
    "    df_features.to_csv(final_output, index=False)\n",
    "    print(f\"\\nFinal processed data saved to {final_output}\")\n",
    "    \n",
    "    # Final validation of translations\n",
    "    final_valid = validate_translations(final_output)\n",
    "    if not final_valid:\n",
    "        print(\"Warning: Some Thai characters remain in the final output. Please update the translation dictionary.\")\n",
    "    else:\n",
    "        print(\"All Thai text successfully translated to English in the final output.\")\n",
    "    \n",
    "    # Print processing summary\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    print(\"\\nPROCESSING SUMMARY:\")\n",
    "    print(f\"Total processing time: {processing_time:.2f} seconds\")\n",
    "    print(f\"Input files: {len(file_paths)}\")\n",
    "    print(f\"Total rows processed: {len(df)}\")\n",
    "    print(f\"Final output rows: {len(df_features)}\")\n",
    "    print(f\"Final output columns: {len(df_features.columns)}\")\n",
    "    \n",
    "    # Print column information\n",
    "    print(\"\\nFinal dataset columns:\")\n",
    "    for col in sorted(df_features.columns):\n",
    "        print(f\"- {col}\")\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "UWB BLE DATA ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Loading processed data...\n",
      "Loaded 1284 rows of data\n",
      "\n",
      "Analyzing position distribution...\n",
      "Saved position heatmap to analysis_output/position_heatmap.png\n",
      "\n",
      "Identifying stationary periods...\n",
      "Identified 24 stationary periods\n",
      "\n",
      "Clustering locations...\n",
      "Identified 11 location clusters\n",
      "\n",
      "Visualizing clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/y6_ywm857n388x9s5vgswchm0000gn/T/ipykernel_15618/2029886208.py:290: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = plt.cm.get_cmap('tab10', len(clusters_df))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cluster visualization to analysis_output/clusters.png\n",
      "\n",
      "Analyzing transitions between clusters...\n",
      "Saved transition matrix to analysis_output/transitions.png\n",
      "\n",
      "Analyzing speed distribution...\n",
      "Saved speed distribution analysis to analysis_output/speed_distribution.png\n",
      "\n",
      "Analyzing data by zone...\n",
      "Saved zone analysis to analysis_output/zone_analysis.png\n",
      "\n",
      "Applying dimensionality reduction...\n",
      "Saved PCA visualization to analysis_output/pca_visualization.png\n",
      "PCA Explained variance: PC1=29.39%, PC2=17.46%\n",
      "\n",
      "Analysis complete. All outputs saved to the 'analysis_output' directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/y6_ywm857n388x9s5vgswchm0000gn/T/ipykernel_15618/2029886208.py:512: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = plt.cm.get_cmap('tab10', len(clusters) if -1 not in clusters else len(clusters)-1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "UWB BLE Data Analysis\n",
    "\n",
    "This script provides functions for analyzing the processed UWB BLE data,\n",
    "including visualization, clustering, and statistical analysis.\n",
    "\n",
    "Usage:\n",
    "    python data_analysis.py\n",
    "\n",
    "Requirements:\n",
    "    - pandas\n",
    "    - numpy\n",
    "    - matplotlib\n",
    "    - seaborn\n",
    "    - scikit-learn\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import os\n",
    "\n",
    "def load_processed_data(file_path='processed_data/uwb_processed_final.csv'):\n",
    "    \"\"\"\n",
    "    Load the processed UWB data\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the processed data file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded data\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def plot_position_heatmap(df, output_path='analysis_output/position_heatmap.png'):\n",
    "    \"\"\"\n",
    "    Create a heatmap of positions\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        output_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Use the Kalman filtered positions if available\n",
    "    x_col = 'x_kalman' if 'x_kalman' in df.columns else 'x'\n",
    "    y_col = 'y_kalman' if 'y_kalman' in df.columns else 'y'\n",
    "    \n",
    "    # Create heatmap\n",
    "    heatmap, xedges, yedges = np.histogram2d(df[x_col], df[y_col], bins=50)\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "    \n",
    "    plt.imshow(heatmap.T, extent=extent, origin='lower', cmap='viridis')\n",
    "    plt.colorbar(label='Frequency')\n",
    "    plt.title('Position Heatmap')\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved position heatmap to {output_path}\")\n",
    "\n",
    "def visualize_movement_over_time(df, output_path='analysis_output/movement_animation.mp4', \n",
    "                              max_frames=300, interval=50):\n",
    "    \"\"\"\n",
    "    Create an animation of movement over time\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        output_path (str): Path to save the animation\n",
    "        max_frames (int): Maximum number of frames to include\n",
    "        interval (int): Interval between frames in milliseconds\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Use the Kalman filtered positions if available\n",
    "    x_col = 'x_kalman' if 'x_kalman' in df.columns else 'x'\n",
    "    y_col = 'y_kalman' if 'y_kalman' in df.columns else 'y'\n",
    "    \n",
    "    # Subsample data if too large\n",
    "    if len(df) > max_frames:\n",
    "        step = len(df) // max_frames\n",
    "        df_anim = df.iloc[::step].reset_index(drop=True)\n",
    "    else:\n",
    "        df_anim = df.reset_index(drop=True)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Set axis limits\n",
    "    ax.set_xlim(df_anim[x_col].min() - 1, df_anim[x_col].max() + 1)\n",
    "    ax.set_ylim(df_anim[y_col].min() - 1, df_anim[y_col].max() + 1)\n",
    "    \n",
    "    # Initialize line and points\n",
    "    line, = ax.plot([], [], 'r-', linewidth=1.5, alpha=0.7)\n",
    "    point, = ax.plot([], [], 'bo', markersize=8)\n",
    "    \n",
    "    # Set up text for time display\n",
    "    time_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
    "    \n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        point.set_data([], [])\n",
    "        time_text.set_text('')\n",
    "        return line, point, time_text\n",
    "    \n",
    "    def animate(i):\n",
    "        line.set_data(df_anim[x_col][:i+1], df_anim[y_col][:i+1])\n",
    "        point.set_data(df_anim[x_col][i], df_anim[y_col][i])\n",
    "        \n",
    "        # Show time information if available\n",
    "        if 'datetime' in df_anim.columns:\n",
    "            time_text.set_text(f'Time: {df_anim[\"datetime\"].iloc[i]}')\n",
    "        else:\n",
    "            time_text.set_text(f'Frame: {i}')\n",
    "            \n",
    "        return line, point, time_text\n",
    "    \n",
    "    # Create animation\n",
    "    anim = FuncAnimation(fig, animate, init_func=init,\n",
    "                         frames=len(df_anim), interval=interval, blit=True)\n",
    "    \n",
    "    # Save animation\n",
    "    anim.save(output_path, writer='ffmpeg', fps=30)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved movement animation to {output_path}\")\n",
    "\n",
    "def identify_stationary_points(df, speed_threshold=0.1, min_duration=5):\n",
    "    \"\"\"\n",
    "    Identify periods when the subject was stationary\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        speed_threshold (float): Maximum speed to consider stationary\n",
    "        min_duration (int): Minimum number of consecutive points to consider a stationary period\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with stationary periods identified\n",
    "    \"\"\"\n",
    "    # Tag points with speed below threshold as potentially stationary\n",
    "    df_stat = df.copy()\n",
    "    if 'speed' in df.columns:\n",
    "        df_stat['is_stationary'] = df['speed'] < speed_threshold\n",
    "    else:\n",
    "        # Calculate speed if not already in dataframe\n",
    "        if 'x_kalman' in df.columns and 'y_kalman' in df.columns and 'time_seconds' in df.columns:\n",
    "            dx = df['x_kalman'].diff()\n",
    "            dy = df['y_kalman'].diff()\n",
    "            dt = df['time_seconds'].diff()\n",
    "            \n",
    "            # Calculate speed (where dt > 0)\n",
    "            mask = dt > 0\n",
    "            speed = np.zeros(len(df))\n",
    "            speed[mask] = np.sqrt(dx[mask]**2 + dy[mask]**2) / dt[mask]\n",
    "            \n",
    "            df_stat['speed'] = speed\n",
    "            df_stat['is_stationary'] = speed < speed_threshold\n",
    "    \n",
    "    # Group consecutive stationary points\n",
    "    df_stat['stationary_group'] = (df_stat['is_stationary'].shift(1) != df_stat['is_stationary']).cumsum()\n",
    "    \n",
    "    # Find groups that are stationary and have at least min_duration points\n",
    "    stat_groups = df_stat[df_stat['is_stationary']].groupby('stationary_group').filter(lambda x: len(x) >= min_duration)\n",
    "    \n",
    "    # Get unique stationary group IDs that meet the criteria\n",
    "    valid_groups = stat_groups['stationary_group'].unique()\n",
    "    \n",
    "    # Identify all points in these valid stationary groups\n",
    "    df_stat['valid_stationary'] = df_stat['stationary_group'].isin(valid_groups)\n",
    "    \n",
    "    # Calculate center of each stationary cluster\n",
    "    stationary_clusters = []\n",
    "    \n",
    "    for group_id in valid_groups:\n",
    "        group_data = df_stat[df_stat['stationary_group'] == group_id]\n",
    "        \n",
    "        # Use Kalman filtered coordinates if available\n",
    "        x_col = 'x_kalman' if 'x_kalman' in df.columns else 'x'\n",
    "        y_col = 'y_kalman' if 'y_kalman' in df.columns else 'y'\n",
    "        \n",
    "        cluster = {\n",
    "            'group_id': group_id,\n",
    "            'center_x': group_data[x_col].mean(),\n",
    "            'center_y': group_data[y_col].mean(),\n",
    "            'start_time': group_data['time_seconds'].min(),\n",
    "            'end_time': group_data['time_seconds'].max(),\n",
    "            'duration': group_data['time_seconds'].max() - group_data['time_seconds'].min(),\n",
    "            'point_count': len(group_data)\n",
    "        }\n",
    "        \n",
    "        stationary_clusters.append(cluster)\n",
    "    \n",
    "    # Create a dataframe for the stationary clusters\n",
    "    stationary_df = pd.DataFrame(stationary_clusters)\n",
    "    \n",
    "    return df_stat, stationary_df\n",
    "\n",
    "def cluster_locations(df, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Cluster positions to identify significant locations\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        eps (float): Maximum distance between two points to be considered in the same cluster\n",
    "        min_samples (int): Minimum number of points to form a dense region\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame: Original dataframe with cluster labels, and cluster summary\n",
    "    \"\"\"\n",
    "    # Use Kalman filtered positions if available\n",
    "    x_col = 'x_kalman' if 'x_kalman' in df.columns else 'x'\n",
    "    y_col = 'y_kalman' if 'y_kalman' in df.columns else 'y'\n",
    "    \n",
    "    # Extract position coordinates\n",
    "    X = df[[x_col, y_col]].values\n",
    "    \n",
    "    # Apply DBSCAN clustering\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
    "    \n",
    "    # Add cluster labels to the dataframe\n",
    "    df_clustered = df.copy()\n",
    "    df_clustered['cluster'] = db.labels_\n",
    "    \n",
    "    # Calculate cluster statistics\n",
    "    clusters = []\n",
    "    \n",
    "    for cluster_id in sorted(set(db.labels_)):\n",
    "        if cluster_id == -1:\n",
    "            continue  # Skip noise points\n",
    "            \n",
    "        cluster_points = df_clustered[df_clustered['cluster'] == cluster_id]\n",
    "        \n",
    "        cluster = {\n",
    "            'cluster_id': cluster_id,\n",
    "            'center_x': cluster_points[x_col].mean(),\n",
    "            'center_y': cluster_points[y_col].mean(),\n",
    "            'point_count': len(cluster_points),\n",
    "            'percentage': len(cluster_points) / len(df) * 100\n",
    "        }\n",
    "        \n",
    "        # Add zone information if available\n",
    "        if 'inExpectedZoneName' in df.columns:\n",
    "            zone_counts = cluster_points['inExpectedZoneName'].value_counts()\n",
    "            most_common_zone = zone_counts.idxmax() if not zone_counts.empty else 'Unknown'\n",
    "            zone_percentage = zone_counts.max() / len(cluster_points) * 100 if not zone_counts.empty else 0\n",
    "            \n",
    "            cluster['most_common_zone'] = most_common_zone\n",
    "            cluster['zone_percentage'] = zone_percentage\n",
    "        \n",
    "        clusters.append(cluster)\n",
    "    \n",
    "    # Create a dataframe for the clusters\n",
    "    clusters_df = pd.DataFrame(clusters)\n",
    "    \n",
    "    return df_clustered, clusters_df\n",
    "\n",
    "def visualize_clusters(df_clustered, clusters_df, output_path='analysis_output/clusters.png'):\n",
    "    \"\"\"\n",
    "    Visualize the identified clusters\n",
    "    \n",
    "    Args:\n",
    "        df_clustered (pd.DataFrame): Dataframe with cluster labels\n",
    "        clusters_df (pd.DataFrame): Dataframe with cluster information\n",
    "        output_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Use the Kalman filtered positions if available\n",
    "    x_col = 'x_kalman' if 'x_kalman' in df_clustered.columns else 'x'\n",
    "    y_col = 'y_kalman' if 'y_kalman' in df_clustered.columns else 'y'\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot noise points (cluster = -1)\n",
    "    noise = df_clustered[df_clustered['cluster'] == -1]\n",
    "    plt.scatter(noise[x_col], noise[y_col], c='lightgray', label='Noise', alpha=0.3, s=10)\n",
    "    \n",
    "    # Generate a color map for clusters\n",
    "    cmap = plt.cm.get_cmap('tab10', len(clusters_df))\n",
    "    \n",
    "    # Plot each cluster with a different color\n",
    "    for i, (_, cluster) in enumerate(clusters_df.iterrows()):\n",
    "        cluster_id = cluster['cluster_id']\n",
    "        cluster_points = df_clustered[df_clustered['cluster'] == cluster_id]\n",
    "        \n",
    "        plt.scatter(cluster_points[x_col], cluster_points[y_col], \n",
    "                   c=[cmap(i)], label=f'Cluster {cluster_id}', alpha=0.7, s=30)\n",
    "        \n",
    "        # Add cluster centroid and annotation\n",
    "        plt.scatter(cluster['center_x'], cluster['center_y'], \n",
    "                   marker='X', c='red', s=100, edgecolor='black')\n",
    "        \n",
    "        # Add annotation\n",
    "        if 'most_common_zone' in cluster:\n",
    "            annotation = f\"{cluster['most_common_zone']}\"\n",
    "        else:\n",
    "            annotation = f\"Cluster {cluster_id}\"\n",
    "            \n",
    "        plt.annotate(annotation, \n",
    "                    (cluster['center_x'], cluster['center_y']),\n",
    "                    xytext=(10, 10),\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=10,\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', fc='yellow', alpha=0.7))\n",
    "    \n",
    "    plt.title('Location Clusters')\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved cluster visualization to {output_path}\")\n",
    "\n",
    "def analyze_transitions(df_clustered, output_path='analysis_output/transitions.png'):\n",
    "    \"\"\"\n",
    "    Analyze transitions between clusters\n",
    "    \n",
    "    Args:\n",
    "        df_clustered (pd.DataFrame): Dataframe with cluster labels\n",
    "        output_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Get non-noise clusters\n",
    "    clusters = sorted(set(df_clustered['cluster']))\n",
    "    if -1 in clusters:\n",
    "        clusters.remove(-1)\n",
    "    \n",
    "    if not clusters:\n",
    "        print(\"No clusters to analyze transitions between.\")\n",
    "        return\n",
    "    \n",
    "    # Create transition matrix\n",
    "    n_clusters = len(clusters)\n",
    "    transition_matrix = np.zeros((n_clusters, n_clusters))\n",
    "    \n",
    "    # Count transitions\n",
    "    prev_cluster = df_clustered['cluster'].iloc[0]\n",
    "    prev_cluster_idx = clusters.index(prev_cluster) if prev_cluster in clusters else None\n",
    "    \n",
    "    for cluster in df_clustered['cluster'][1:]:\n",
    "        if cluster in clusters and prev_cluster in clusters:\n",
    "            curr_cluster_idx = clusters.index(cluster)\n",
    "            prev_cluster_idx = clusters.index(prev_cluster)\n",
    "            \n",
    "            if prev_cluster_idx is not None and curr_cluster_idx != prev_cluster_idx:\n",
    "                transition_matrix[prev_cluster_idx, curr_cluster_idx] += 1\n",
    "        \n",
    "        prev_cluster = cluster\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(transition_matrix, annot=True, fmt='g', cmap='viridis',\n",
    "               xticklabels=[f'Cluster {c}' for c in clusters],\n",
    "               yticklabels=[f'Cluster {c}' for c in clusters])\n",
    "    \n",
    "    plt.title('Transition Matrix Between Clusters')\n",
    "    plt.xlabel('To Cluster')\n",
    "    plt.ylabel('From Cluster')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved transition matrix to {output_path}\")\n",
    "\n",
    "def analyze_speed_distribution(df, output_path='analysis_output/speed_distribution.png'):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of speeds\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        output_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    if 'speed' not in df.columns:\n",
    "        print(\"Speed column not found in dataframe.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create subplot for histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df['speed'].dropna(), kde=True)\n",
    "    plt.title('Speed Distribution')\n",
    "    plt.xlabel('Speed')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Create subplot for boxplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(y=df['speed'].dropna())\n",
    "    plt.title('Speed Boxplot')\n",
    "    plt.ylabel('Speed')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved speed distribution analysis to {output_path}\")\n",
    "\n",
    "def analyze_by_zone(df, output_path='analysis_output/zone_analysis.png'):\n",
    "    \"\"\"\n",
    "    Analyze data by zone\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        output_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    if 'inExpectedZoneName' not in df.columns:\n",
    "        print(\"Zone information not found in dataframe.\")\n",
    "        return\n",
    "    \n",
    "    # Count points per zone\n",
    "    zone_counts = df['inExpectedZoneName'].value_counts()\n",
    "    \n",
    "    # Calculate time spent in each zone\n",
    "    if 'time_seconds' in df.columns:\n",
    "        zone_times = df.groupby('inExpectedZoneName')['time_seconds'].agg(['min', 'max'])\n",
    "        zone_times['duration'] = zone_times['max'] - zone_times['min']\n",
    "        \n",
    "        # Calculate average speed per zone if available\n",
    "        if 'speed' in df.columns:\n",
    "            zone_speeds = df.groupby('inExpectedZoneName')['speed'].mean()\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot zone counts\n",
    "    plt.subplot(2, 1, 1)\n",
    "    zone_counts.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Points per Zone')\n",
    "    plt.xlabel('Zone')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Plot zone durations if available\n",
    "    if 'time_seconds' in df.columns:\n",
    "        plt.subplot(2, 1, 2)\n",
    "        zone_times['duration'].plot(kind='bar', color='salmon')\n",
    "        plt.title('Time Spent per Zone')\n",
    "        plt.xlabel('Zone')\n",
    "        plt.ylabel('Duration (seconds)')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved zone analysis to {output_path}\")\n",
    "\n",
    "def dimensionality_reduction(df, output_path='analysis_output/pca_visualization.png'):\n",
    "    \"\"\"\n",
    "    Apply PCA for dimensionality reduction and visualization\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        output_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Select numerical features for PCA\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Exclude specific columns that shouldn't be used for PCA\n",
    "    exclude_cols = ['id', 'timestamp', 'time_seconds', 'floor', 'cluster']\n",
    "    features = [col for col in numerical_cols if col not in exclude_cols]\n",
    "    \n",
    "    if len(features) < 3:\n",
    "        print(\"Not enough numerical features for PCA.\")\n",
    "        return\n",
    "    \n",
    "    # Handle NaN values\n",
    "    X = df[features].fillna(0).values\n",
    "    \n",
    "    # Standardize features\n",
    "    X_std = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(X_std)\n",
    "    \n",
    "    # Create dataframe with principal components\n",
    "    pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "    \n",
    "    # Add color information if clusters exist\n",
    "    if 'cluster' in df.columns:\n",
    "        pca_df['cluster'] = df['cluster'].values\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    if 'cluster' in pca_df.columns:\n",
    "        clusters = sorted(set(pca_df['cluster']))\n",
    "        cmap = plt.cm.get_cmap('tab10', len(clusters) if -1 not in clusters else len(clusters)-1)\n",
    "        \n",
    "        # Plot each cluster\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            if cluster == -1:\n",
    "                # Plot noise points\n",
    "                points = pca_df[pca_df['cluster'] == cluster]\n",
    "                plt.scatter(points['PC1'], points['PC2'], c='lightgray', label='Noise', alpha=0.5, s=10)\n",
    "            else:\n",
    "                # Plot cluster points\n",
    "                points = pca_df[pca_df['cluster'] == cluster]\n",
    "                plt.scatter(points['PC1'], points['PC2'], c=[cmap(i)], label=f'Cluster {cluster}', alpha=0.7)\n",
    "        \n",
    "        plt.legend()\n",
    "    else:\n",
    "        # If no clusters, plot all points the same\n",
    "        plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.7)\n",
    "    \n",
    "    # Add explained variance information\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    plt.xlabel(f'Principal Component 1 ({explained_variance[0]:.2%} variance)')\n",
    "    plt.ylabel(f'Principal Component 2 ({explained_variance[1]:.2%} variance)')\n",
    "    plt.title('PCA of UWB Data')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved PCA visualization to {output_path}\")\n",
    "    print(f\"PCA Explained variance: PC1={explained_variance[0]:.2%}, PC2={explained_variance[1]:.2%}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run all analyses\n",
    "    \"\"\"\n",
    "    # Set paths\n",
    "    input_file = 'processed_data/uwb_processed_final.csv'\n",
    "    output_dir = 'analysis_output'\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"UWB BLE DATA ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load processed data\n",
    "    print(\"\\nLoading processed data...\")\n",
    "    df = load_processed_data(input_file)\n",
    "    print(f\"Loaded {len(df)} rows of data\")\n",
    "    \n",
    "    # Analyze position distribution\n",
    "    print(\"\\nAnalyzing position distribution...\")\n",
    "    plot_position_heatmap(df)\n",
    "    \n",
    "    # Identify stationary points\n",
    "    print(\"\\nIdentifying stationary periods...\")\n",
    "    df_stationary, stationary_clusters = identify_stationary_points(df)\n",
    "    print(f\"Identified {len(stationary_clusters)} stationary periods\")\n",
    "    \n",
    "    # Cluster locations\n",
    "    print(\"\\nClustering locations...\")\n",
    "    df_clustered, clusters = cluster_locations(df)\n",
    "    print(f\"Identified {len(clusters)} location clusters\")\n",
    "    \n",
    "    # Visualize clusters\n",
    "    print(\"\\nVisualizing clusters...\")\n",
    "    visualize_clusters(df_clustered, clusters)\n",
    "    \n",
    "    # Analyze transitions\n",
    "    print(\"\\nAnalyzing transitions between clusters...\")\n",
    "    analyze_transitions(df_clustered)\n",
    "    \n",
    "    # Analyze speed distribution\n",
    "    print(\"\\nAnalyzing speed distribution...\")\n",
    "    analyze_speed_distribution(df)\n",
    "    \n",
    "    # Analyze by zone\n",
    "    print(\"\\nAnalyzing data by zone...\")\n",
    "    analyze_by_zone(df)\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    print(\"\\nApplying dimensionality reduction...\")\n",
    "    dimensionality_reduction(df_clustered)\n",
    "    \n",
    "    print(\"\\nAnalysis complete. All outputs saved to the 'analysis_output' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
